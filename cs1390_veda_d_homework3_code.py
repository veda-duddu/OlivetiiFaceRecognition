# -*- coding: utf-8 -*-
"""CS1390_Veda_D - Homework 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c86ueQse433rog2rAkcO4Qb1HsCzegT7

We will begin by loading the packages required for facial recognition via PCA on the oivetti faceset. We will be training the model via support vector machine.
"""

#@title Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

#scikit packages 
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.datasets import fetch_olivetti_faces
from sklearn import metrics
from sklearn.metrics import confusion_matrix

"""**Exploring the datasets**


As we don't exactly know what is within the datasets, it will be key to explore the datasets and we will do that by beginning to printing the dataset and plotting the images
"""

faces = fetch_olivetti_faces() #downloading a dataset
print(faces.DESCR)

"""**My Understanding what does the dataset contain**

After exploring the data, I have understood that there are 40 classes where each class represents an induvidual's images. Each class has 10 images as the total sample is full of 400 images. The images parameter of the dataset contains the 400 images and each image's information is stored in a 2-D array of 64*x*64 pixels. The data array is the previous 2-D array converted to a 1-D array, and therefore contains 4096 elements for every image. The target parameter contains the mapping of the image and as there are 400 images, therefore there are 400 targets.
"""

#@title Creation of Variables
features = faces.images.reshape((faces.images.shape[0],faces.images.shape[1]*faces.images.shape[2]))
target = faces.target

"""It is imperative to check out the elements to understand what form of cleaning it may require. Therefore, we will print a couple of elements and take a look."""

fig, plots = plt.subplots(nrows = 10, ncols = 10, figsize = (20,25))
plots = plots.flatten()
for i in range(100):
  plots[i].imshow(faces.images[i], cmap="gray")
  plots[i].set_xticks([])
  plots[i].set_yticks([])
  plots[i].set_title("face id:{}".format(i))# printing the respective face number in the sample

#@title Prinitng the faces of each induvidual
no_people = 40
no_elements = 10
fig, plt1 = plt.subplots(nrows=4, ncols=10, figsize=(20, 25))
plt1 = plt1.flatten()
for j in range(0,no_people):
  identity = j*no_elements
  pid = target[identity]
  plt1[pid].imshow(faces.images[identity], cmap='gray');
  plt1[pid].set_title("%d"%pid);
  plt1[pid].set_xticks([]);
  plt1[pid].set_yticks([])

"""**What we understand by going through the sample**

Looking over the some of the samples, we can see the dataset does not need any cleaning as the faces are centered and have a good lighting. Thse samples therefore don't really need cleaning. 
"""

#@title Creation of Training data and Testing data
ratio = 0.20
features_train, features_test, target_train, target_test=train_test_split(features, target, test_size=ratio, stratify=target, random_state=0)

rows = len(features[0])
print(rows)

"""**Finding the number of principal components**

It is vital the total number of components in our pricnipal component anaylis as we want to find the ideal number of components. Too less and we have not spanned the set of components and too many components ensures unnecessary calculation. Therefore, by plotting a graph between expected variance and nummber of components, we can pick the lowest number of principal components required for a decent accuacy in our learning. The expected variance is the n_components largest eigenvalues of the covariance matrix of features .
"""

#@title PCA compenent analysis -Finding the number of components
principal_component_analysis = PCA()
principal_component_analysis.fit(features)
plt.figure(1, figsize=(12,10))
plt.plot(principal_component_analysis.explained_variance_, linewidth=2)
 
plt.xlabel('Components')
plt.ylabel('Explained Variaces')
plt.show()

"""**What we chose as the number of components**

We will be choosing n = 110 components as it seems like that the expected variance barely changes with more than 110 components. The expected variance describes the vairance between the numper of components that are selected
"""

#@title Apply PCA on the dataset to gain the eigen vectors
no_components = 110
principal_component_analysis = PCA(n_components=no_components)
principal_component_analysis.fit(features_train)
no_eigen_faces = len(principal_component_analysis.components_)
eigen_faces = principal_component_analysis.components_.reshape((no_eigen_faces, faces.images.shape[1], faces.images.shape[2]))

columns = 10
rows = int(no_eigen_faces/columns)
fig, ax = plt.subplots(nrows = rows, ncols = columns, figsize = (20,25))
ax = ax.flatten()
for i in range(no_eigen_faces):
  ax[i].imshow(eigen_faces[i],cmap="gray")
  #removing axes
  ax[i].set_xticks([])
  ax[i].set_yticks([])
  ax[i].set_title("eigen id:{}".format(i))# printing the respected eigen face number

#@title Using PCA and SVM to train and test data
features_train_principal_component_analysis = principal_component_analysis.transform(features_train)
features_test_principal_component_analysis = principal_component_analysis.transform(features_test)
support_vector_machine = SVC()
support_vector_machine.fit(features_train_principal_component_analysis,target_train)
target_prediction = support_vector_machine.predict(features_test_principal_component_analysis)

accuracy = metrics.accuracy_score(target_test, target_prediction)
print("When the model with traininng test set ratio being set to", ratio, "with no of principal compenents equal to", no_components, "has an accuracy of",  accuracy)

#@title Taking a look at the target values and predicted values
print("If you want to see the whole target prediction and test array, uncomment the following two lines")
#print(target_prediction)
#print(target_test)

#This counter variable will keep track of the number of time the model printed incorrectly
counter = 0
#This array will tell us at which indices it was printed incorrectly
Values_predicted_incorrectly = []
for i in range(len(target_prediction)):
  if(target_prediction[i]!=target_test[i]):
    counter = counter + 1
    Values_predicted_incorrectly.append(i)
print("The model predicts", counter, "incorrectly out of the", len(target_test), "cases")
print("The values at the following indices were incorrectly predicted", Values_predicted_incorrectly)

print("Now if you want to see what the values were printed as for the incorrect ones and what they were supposed to be, uncomment the following loop")

for i in range(len(Values_predicted_incorrectly)):
  indice = Values_predicted_incorrectly[i]
  print("At the indice value",indice,"this was the predicted value", target_prediction[indice], " while it was supposed to be", target_test[indice])

"""Next we will print a classification report. This classification report gives us details such as the number of accurate positive predictions given by precision, and recall being the value of the classifier's ability to find all positive instances. The f1-score is the weighted harmonic of the precision and recall value. I will be using the classification report vector. It is breaking down a larger and bigger confusion matrix into more simple terms by using the classififcation report function. """

#@title Classification report via the basis of confusion matrix
report = metrics.classification_report(target_test, target_prediction)
print(report)

"""The following contains the confusion matrix printed out as it was explicitly mentioned in the question though the above classification report does utilize the confusion matrix."""

confusion_accuracy = confusion_matrix(target_test, target_prediction)
print(confusion_accuracy)